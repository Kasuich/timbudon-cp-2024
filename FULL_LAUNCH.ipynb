{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полный запуск решения\n",
    "\n",
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731189501.109606  842277 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731189501.112678  842277 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from ultralytics import YOLO\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import cv2\n",
    "from dataclasses import dataclass\n",
    "import abc\n",
    "from paddleocr import PaddleOCR\n",
    "from sklearn.metrics import accuracy_score, pairwise_distances\n",
    "\n",
    "config_path = 'config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для дальнейшей работы, нужно скачать веса, и положить в папку weights\n",
    "\n",
    "https://drive.google.com/file/d/154jS1mS7ca43gm1eSu_DhzP7Y7f7eCHU/view?usp=sharing  \n",
    "https://drive.google.com/file/d/1Rssq6iwe8ExxcSG7hnjz1UZiieUDkwVh/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение Эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(output_folder: str) -> List[np.ndarray]:\n",
    "    \n",
    "    frame_files = sorted(os.listdir(output_folder))\n",
    "    frames = []\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(output_folder, frame_file)\n",
    "        if os.path.isfile(frame_path) and frame_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "            try:\n",
    "                img = Image.open(frame_path)\n",
    "                frames.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                logger.eror(f\"Error opening {frame_path}: {e}\")\n",
    "        else:\n",
    "            logger.eror(f\"Skipping directory or non-image file: {frame_path}\")\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def save_embeddings(embeddings, filename, output_folder):\n",
    "    output_path = Path(output_folder) / f\"{filename}.pkl\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    logger.info(f\"Saved embeddings to {output_path}\")\n",
    "\n",
    "def vectorize_images(images: List[np.ndarray], model: SentenceTransformer) -> List[np.ndarray]:\n",
    "    return [model.encode(Image.fromarray(img)) for img in tqdm(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-10 00:58:26.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mLoaded configuration from config.yaml\u001b[0m\n",
      "\u001b[32m2024-11-10 00:58:26.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLoaded test: test/images\u001b[0m\n",
      "\u001b[32m2024-11-10 00:58:26.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mLoaded train: train/images\u001b[0m\n",
      "\u001b[32m2024-11-10 00:58:28.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mLoaded model: clip-ViT-B-16\u001b[0m\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.31it/s]\n",
      "\u001b[32m2024-11-10 00:58:32.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_embeddings\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mSaved embeddings to weights/embed/test_emb.pkl\u001b[0m\n",
      "\u001b[32m2024-11-10 00:58:32.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mSaved embeddings for test and train images.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "logger.info(\"Loaded configuration from {}\", config_path)\n",
    "\n",
    "test_images = load_images_from_folder(config['test_images_folder'])\n",
    "logger.info(\"Loaded test: {}\", config['test_images_folder'])\n",
    "train_images = load_images_from_folder(config['train_images_folder'])\n",
    "logger.info(\"Loaded train: {}\", config['train_images_folder'])\n",
    "\n",
    "model = SentenceTransformer(config['model_name'])\n",
    "logger.info(\"Loaded model: {}\", config['model_name'])\n",
    "\n",
    "test_embeddings = vectorize_images(test_images, model)\n",
    "\n",
    "save_embeddings(test_embeddings, 'test_emb', config['emb_output_folder'])\n",
    "\n",
    "logger.info(\"Saved embeddings for test and train images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посик похожих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_folder(folder: str) -> tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"Загрузка Эмбеддингов\"\"\"\n",
    "    test_embeddings = []\n",
    "    train_embeddings = []\n",
    "    for filename in os.listdir(folder):\n",
    "        emb_path = os.path.join(folder, filename)\n",
    "\n",
    "        if os.path.isfile(emb_path):\n",
    "            with open(emb_path, 'rb') as f:\n",
    "                embedding = pickle.load(f)\n",
    "\n",
    "                if 'test' in filename.lower():\n",
    "                    test_embeddings.append(embedding)\n",
    "                elif 'train' in filename.lower():\n",
    "                    train_embeddings.append(embedding)\n",
    "\n",
    "    return test_embeddings, train_embeddings\n",
    "\n",
    "\n",
    "def load_image_filenames(images_folder: str) -> List[str]:\n",
    "    \"\"\"Загружает имена файлов изображений из указанной папки.\"\"\"\n",
    "    image_filenames = []\n",
    "    for filename in sorted(os.listdir(images_folder)):\n",
    "        if filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif', 'bbox', 'txt')):  # Фильтруем по типу изображения\n",
    "            image_filenames.append(filename)\n",
    "    return image_filenames\n",
    "\n",
    "def find_nearest_neighbors(test_embeddings: List[np.ndarray], \n",
    "                           train_embeddings: List[np.ndarray], \n",
    "                           n_neighbors: int, \n",
    "                           threshold: float) -> List[List[int]]:\n",
    "    test_embeddings = np.array(test_embeddings)[0]\n",
    "    train_embeddings = np.array(train_embeddings)[0]\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree')\n",
    "    nn.fit(train_embeddings)  \n",
    "    \n",
    "    neighbors_indices = []\n",
    "    for test_emb in test_embeddings:\n",
    "        distances, indices = nn.kneighbors([test_emb])  \n",
    "        valid_indices = [idx for dist, idx in zip(distances[0], indices[0]) if dist < threshold]\n",
    "        \n",
    "        if valid_indices:\n",
    "            neighbors_indices.append(valid_indices[0])\n",
    "        else:\n",
    "            neighbors_indices.append(None)\n",
    "\n",
    "    return neighbors_indices\n",
    "\n",
    "\n",
    "def load_labels(labels_folder: str, file_extension: str, train_filenames: List[str]) -> List[str]:\n",
    "    \"\"\"Загружает метки из папки, фильтруя по расширению файла и проверяя, что имя файла присутствует в списке train_filenames.\"\"\"\n",
    "    labels = []\n",
    "    train_filenames_base = [filename.split('.')[0] for filename in train_filenames]\n",
    "    \n",
    "    for filename in sorted(os.listdir(labels_folder)):\n",
    "        if filename.split('.')[-1] == file_extension.lstrip('.') and filename.split('.')[0] in train_filenames_base:\n",
    "            with open(os.path.join(labels_folder, filename), 'r') as file:\n",
    "                # Читаем строки и добавляем `\\n`, если его нет\n",
    "                content = ''.join(line if line.endswith('\\n') else line + '\\n' for line in file.readlines())\n",
    "                labels.append(content)\n",
    "                \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-10 01:03:38.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mLoaded configuration from config.yaml\u001b[0m\n",
      "\u001b[32m2024-11-10 01:03:38.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEmbeddings were read\u001b[0m\n",
      "\u001b[32m2024-11-10 01:03:38.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mtrain_labels and train_labels_with_text were read\u001b[0m\n",
      "\u001b[32m2024-11-10 01:03:38.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mTest image filenames were read\u001b[0m\n",
      "\u001b[32m2024-11-10 01:03:38.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mNeighbours were found - [182, 186, 192, 223, 224, 225, 230, 232, 232]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "logger.info(\"Loaded configuration from {}\", config_path)\n",
    "\n",
    "test_embeddings, train_embeddings = load_embeddings_from_folder(config['emb_output_folder'])\n",
    "logger.info(\"Embeddings were read\")\n",
    "\n",
    "test_filenames = load_image_filenames(config['test_images_folder'])\n",
    "train_filenames = load_image_filenames(config['train_images_folder'])\n",
    "\n",
    "train_labels = load_labels(config['train_labels_folder'], '.txt', train_filenames)\n",
    "train_labels_with_text = load_labels(config['train_labels_with_text_folder'], '.bbox', train_filenames)\n",
    "logger.info(\"train_labels and train_labels_with_text were read\")\n",
    "\n",
    "logger.info(\"Test image filenames were read\")\n",
    "\n",
    "n_neighbors = config['n_neighbors']\n",
    "threshold = config['threshold']\n",
    "\n",
    "nearest_neighbors = find_nearest_neighbors(test_embeddings, train_embeddings, n_neighbors, threshold)\n",
    "logger.info(f\"Neighbours were found - {nearest_neighbors}\")\n",
    "results = []\n",
    "for test_idx, neighbors in enumerate(nearest_neighbors):\n",
    "    if neighbors:\n",
    "        neighbor_idx = neighbors \n",
    "        results.append([\n",
    "            test_filenames[test_idx], \n",
    "            train_labels[neighbor_idx], \n",
    "            train_labels_with_text[neighbor_idx]\n",
    "        ])\n",
    "    else:\n",
    "        results.append([\n",
    "            test_filenames[test_idx],  \n",
    "            None,  \n",
    "            None   \n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сегментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation:\n",
    "    def __init__(self, weights_yolo_path: str):\n",
    "        self.model = YOLO(weights_yolo_path)\n",
    "        self.data = {}\n",
    "        \n",
    "    def get_segmentation(self) -> None:\n",
    "        result = self.model(self.image, conf=0.7)\n",
    "        if len(result[0]):\n",
    "            object_masks = np.array(result[0].masks.xy, dtype=object)\n",
    "            self.data[\"masks\"] = object_masks \n",
    "        else:\n",
    "            self.data[\"masks\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR(Segmentation):\n",
    "    def __init__(self, weights_yolo_path: str, image: Image.Image):\n",
    "        super().__init__(weights_yolo_path)\n",
    "        self.ocr = PaddleOCR(use_gpu=True, lang=\"en\")  \n",
    "        self.image = image\n",
    "        self.get_segmentation()\n",
    "        self.crop_one_img()\n",
    "        self.ocr_one_img()\n",
    "\n",
    "    def get_mask(self) -> np.array:\n",
    "        mask = np.zeros((self.image.size[1], self.image.size[0]), dtype=np.uint8)\n",
    "        for object in self.data[\"masks\"]:\n",
    "            points = np.array(\n",
    "                [[x, y] for x, y in object], dtype=np.int32\n",
    "            )\n",
    "            mask = cv2.fillPoly(mask, [points], color=255)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def crop_one_img(self) -> None:\n",
    "        mask = (np.array(self.get_mask()) > 0)\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "        image = self.image * mask\n",
    "        if len(self.data[\"masks\"]):\n",
    "            x = np.array([x for obj in self.data[\"masks\"] for x, y in obj])\n",
    "            y = np.array([y for obj in self.data[\"masks\"] for x, y in obj])\n",
    "            x_min, x_max = int(min(x)), int(max(x))\n",
    "            y_min, y_max = int(min(y)), int(max(y))\n",
    "            self.data[\"crop_img\"] = image[y_min:y_max, x_min:x_max, :]\n",
    "        else:\n",
    "            self.data[\"crop_img\"] = image\n",
    "    \n",
    "    def ocr_one_img(self) -> None:\n",
    "        crop_image = np.array(self.data[\"crop_img\"])\n",
    "        orig_image = np.array(self.image)\n",
    "\n",
    "        result = self.ocr.ocr(crop_image, rec=True)\n",
    "        if result[0]:\n",
    "            self.data[\"rec_crop\"] = [line[1][0] for line in result[0]]\n",
    "        else:\n",
    "            self.data[\"rec_crop\"] = [\"None\"]\n",
    "\n",
    "        result = self.ocr.ocr(orig_image, rec=True)\n",
    "        if result[0]:\n",
    "            self.data[\"rec_orig\"] = [line[1][0] for line in result[0]]\n",
    "        else:\n",
    "            self.data[\"rec_orig\"] = [\"None\"]\n",
    "\n",
    "    def get_text(self) -> Dict[str, List[str]]:\n",
    "        dict_text = {\n",
    "            \"text_orig_img\": self.data[\"rec_orig\"],\n",
    "            \"text_crop_img\": self.data[\"rec_crop\"],\n",
    "        }\n",
    "        return dict_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PredictResult:\n",
    "    raw_text: str = None\n",
    "    # image in bytes with boxes and text on it\n",
    "    pred_img: str = None\n",
    "    # unknow data from excel, None if search_in_data is False\n",
    "    attribute1: str | None = None\n",
    "    attribute2: str | None = None\n",
    "    attribute3: str | None = None\n",
    "\n",
    "class BaseModel(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(\n",
    "        self, image: Image.Image, search_in_data: bool, dist_threshold: float\n",
    "    ) -> PredictResult:\n",
    "        \"\"\"Get predict from ML OCR Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        images : Image.Image\n",
    "            List with images to be predicted\n",
    "        search_in_data : bool\n",
    "            Flag, if true, get missing data from excel file\n",
    "        dist_threshold : float\n",
    "            Distance threshold to cut out unknown images\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PredictResult\n",
    "            If search_in_data is True, returns full data from excel\n",
    "            If False, return only OCR result\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcrBD():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer(\"clip-ViT-B-16\")\n",
    "        self.emb_output_folder = \"embeddings_vit\"\n",
    "        self.test_images_folder = \"test/images\"\n",
    "        self.train_labels_folder = \"train/labels\"\n",
    "        self.train_labels_with_text_folder = \"train/labels_with_text\"\n",
    "        self.config_path = \"config.yaml\"\n",
    "        with open(self.config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        logger.info(\"Loaded configuration from {}\", self.config_path)\n",
    "\n",
    "\n",
    "    def load_embeddings_from_folder(self, folder: str) -> tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        for filename in os.listdir(folder):\n",
    "            emb_path = os.path.join(folder, filename)\n",
    "    \n",
    "            if os.path.isfile(emb_path):\n",
    "                with open(emb_path, 'rb') as f:\n",
    "                    embedding = pickle.load(f)\n",
    "    \n",
    "                    if 'test' in filename.lower():\n",
    "                        test_embeddings = embedding\n",
    "                    elif 'train' in filename.lower():\n",
    "                        train_embeddings = embedding\n",
    "    \n",
    "        return train_embeddings\n",
    "        \n",
    "    def vectorize_img(self, image: Image.Image) -> np.ndarray:\n",
    "        return [self.model.encode(image)]\n",
    "\n",
    "    def load_image_filenames(self, images_folder: str) -> List[str]:\n",
    "        image_filenames = []\n",
    "        for filename in sorted(os.listdir(images_folder)):\n",
    "            if filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif', 'bbox', 'txt')): \n",
    "                image_filenames.append(filename)\n",
    "        return image_filenames\n",
    "\n",
    "    def find_nearest_neighbors(self,\n",
    "                               test_embeddings: List[np.ndarray], \n",
    "                               train_embeddings: List[np.ndarray], \n",
    "                               n_neighbors: int, \n",
    "                               threshold: float) -> List[List[int]]:\n",
    "        test_embeddings = np.array(test_embeddings)\n",
    "        train_embeddings = np.array(train_embeddings)\n",
    "        nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree')\n",
    "        nn.fit(train_embeddings)  \n",
    "        \n",
    "        neighbors_indices = []\n",
    "        for test_emb in test_embeddings:\n",
    "            distances, indices = nn.kneighbors([test_emb])  \n",
    "            valid_indices = [idx for dist, idx in zip(distances[0], indices[0]) if dist > 0 and dist < threshold]\n",
    "            \n",
    "            if valid_indices:\n",
    "                neighbors_indices.append(valid_indices[0])\n",
    "            else:\n",
    "                neighbors_indices.append(None)\n",
    "    \n",
    "        return neighbors_indices \n",
    "        \n",
    "    def load_labels(self, labels_folder: str, file_extension: str, train_filenames: List[str]) -> List[str]:\n",
    "        labels = []\n",
    "        train_filenames_base = [filename.split('.')[0] for filename in train_filenames]\n",
    "        \n",
    "        for filename in sorted(os.listdir(labels_folder)):\n",
    "            if filename.split('.')[-1] == file_extension.lstrip('.') and filename.split('.')[0] in train_filenames_base:\n",
    "                with open(os.path.join(labels_folder, filename), 'r') as file:\n",
    "                    # Читаем строки и добавляем `\\n`, если его нет\n",
    "                    content = ''.join(line if line.endswith('\\n') else line + '\\n' for line in file.readlines())\n",
    "                    labels.append(content)\n",
    "                    \n",
    "        return labels\n",
    "\n",
    "    def predict(self, image: Image.Image, search_in_data: bool, dist_threshold: float) -> PredictResult:\n",
    "        config = self.config\n",
    "    \n",
    "        train_embeddings = self.load_embeddings_from_folder(config['emb_output_folder'])\n",
    "        test_embedings = self.vectorize_img(image)\n",
    "        \n",
    "        logger.info(\"Embeddings were read\")\n",
    "    \n",
    "        test_filenames = self.load_image_filenames(config['test_images_folder'])\n",
    "        train_filenames = self.load_image_filenames(config['train_images_folder'])\n",
    "    \n",
    "        train_labels = self.load_labels(config['train_labels_folder'], '.txt', train_filenames)\n",
    "        train_labels_with_text = self.load_labels(config['train_labels_with_text_folder'], '.bbox', train_filenames)\n",
    "        logger.info(\"train_labels and train_labels_with_text were read\")\n",
    "        \n",
    "        logger.info(\"Test image filenames were read\")\n",
    "    \n",
    "        n_neighbors = config['n_neighbors']\n",
    "        threshold = config['threshold']\n",
    "        nearest_neighbors = self.find_nearest_neighbors(test_embedings, train_embeddings, n_neighbors, threshold)\n",
    "        logger.info(f\"Neighbours were found - {nearest_neighbors}\")\n",
    "        results = []\n",
    "        for test_idx, neighbors in enumerate(nearest_neighbors):\n",
    "            if neighbors:\n",
    "                neighbor_idx = neighbors \n",
    "                results.append([\n",
    "                    test_idx, \n",
    "                    train_labels[neighbor_idx], \n",
    "                    train_labels_with_text[neighbor_idx],\n",
    "                    train_filenames[neighbor_idx],\n",
    "                ])\n",
    "            else:\n",
    "                results.append([\n",
    "                    test_filenames[test_idx],  \n",
    "                    None,  \n",
    "                    None,\n",
    "                    None\n",
    "                ])\n",
    "\n",
    "        df = pd.DataFrame(results, columns=['Test_Embedding', 'Label', 'Label_With_Text', 'Neighbour'])\n",
    "        df[\"Label_With_Text\"] = df[\"Label_With_Text\"].map(lambda x: x[:-1])\n",
    "        df.to_excel(config['output_excel'], index=False)\n",
    "        logger.info(\"Saved results to Excel: {}\", config['output_excel'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcrPipeline(BaseModel):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.weights = './weights/best.pt'\n",
    "\n",
    "    def predict(self, image: Image.Image, search_in_data: bool, dist_threshold: float) -> PredictResult:\n",
    "        ocr = OCR(self.weights, image)\n",
    "        dict_text = ocr.get_text()\n",
    "        model_neighbour = OcrBD()\n",
    "        result = model_neighbour.predict(image, search_in_data=False, dist_threshold=10.5)\n",
    "        res = PredictResult(raw_text = result[\"Label_With_Text\"])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инференс "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../sergey/runs/segment/train3/weights/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_images_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_images_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m], img_path))\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_in_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mOcrPipeline.predict\u001b[0;34m(self, image, search_in_data, dist_threshold)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: Image\u001b[38;5;241m.\u001b[39mImage, search_in_data: \u001b[38;5;28mbool\u001b[39m, dist_threshold: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PredictResult:\n\u001b[0;32m----> 7\u001b[0m     ocr \u001b[38;5;241m=\u001b[39m \u001b[43mOCR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     dict_text \u001b[38;5;241m=\u001b[39m ocr\u001b[38;5;241m.\u001b[39mget_text()\n\u001b[1;32m      9\u001b[0m     model_neighbour \u001b[38;5;241m=\u001b[39m OcrBD()\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mOCR.__init__\u001b[0;34m(self, weights_yolo_path, image)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights_yolo_path: \u001b[38;5;28mstr\u001b[39m, image: Image\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights_yolo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mocr \u001b[38;5;241m=\u001b[39m PaddleOCR(use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;241m=\u001b[39m image\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mSegmentation.__init__\u001b[0;34m(self, weights_yolo_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights_yolo_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_yolo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/ultralytics/models/yolo/model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/ultralytics/engine/model.py:145\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/ultralytics/engine/model.py:285\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    282\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/ultralytics/nn/tasks.py:910\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    909\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/ultralytics/nn/tasks.py:837\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight, safe_only)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/ultralytics/utils/patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/micromamba/envs/new_laaunch/lib/python3.11/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../sergey/runs/segment/train3/weights/best.pt'"
     ]
    }
   ],
   "source": [
    "model = OcrPipeline()\n",
    "ans = pd.DataFrame()\n",
    "for img_path in os.listdir(config[\"test_images_folder\"]):\n",
    "    image = Image.open(os.path.join(config[\"test_images_folder\"], img_path))\n",
    "    result = model.predict(image, search_in_data=False, dist_threshold=10.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
